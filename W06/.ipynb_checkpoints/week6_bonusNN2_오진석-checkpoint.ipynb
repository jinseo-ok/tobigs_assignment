{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "class TwoLayerNet():\n",
    "    \"\"\"\n",
    "    2 Layer Network를 만드려고 합니다.\n",
    "\n",
    "    해당 네트워크는 아래의 구조를 따릅니다.\n",
    "\n",
    "    input - Linear - ReLU - Linear - Softmax\n",
    "\n",
    "    Softmax 결과는 입력 N개의 데이터에 대해 개별 클래스에 대한 확률입니다.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, X, input_size, hidden_size, output_size, std=1e-4):\n",
    "         \"\"\"\n",
    "         네트워크에 필요한 가중치들을 initialization합니다.\n",
    "         initialized by random values\n",
    "         해당 가중치들은 self.params 라는 Dictionary에 담아둡니다.\n",
    "\n",
    "         input_size: 데이터의 변수 개수 - D\n",
    "         hidden_size: 히든 층의 H 개수 - H\n",
    "         output_size: 클래스 개수 - C\n",
    "\n",
    "         \"\"\"\n",
    "\n",
    "         self.params = {}\n",
    "         self.params[\"W1\"] = std * np.random.randn(input_size, hidden_size)\n",
    "         self.params[\"b1\"] = np.random.randn(hidden_size)\n",
    "         self.params[\"W2\"] = std * np.random.randn(hidden_size, output_size)\n",
    "         self.params[\"b2\"] = np.random.randn(output_size)\n",
    "\n",
    "    def forward(self, X, y=None):\n",
    "\n",
    "        \"\"\"\n",
    "\n",
    "        X: input 데이터 (N, D)\n",
    "        y: 레이블 (N,)\n",
    "\n",
    "        Linear - ReLU - Linear - Softmax - CrossEntropy Loss\n",
    "\n",
    "        y가 주어지지 않으면 Softmax 결과 p와 Activation 결과 a를 return합니다. p와 a 모두 backward에서 미분할때 사용합니다.\n",
    "        y가 주어지면 CrossEntropy Error를 return합니다.\n",
    "\n",
    "        \"\"\"\n",
    "        # W1 == (D,H), b1 == (H,)\n",
    "        W1, b1 = self.params[\"W1\"], self.params[\"b1\"]\n",
    "        # W2 == (H,C), b2 == (C,)\n",
    "        W2, b2 = self.params[\"W2\"], self.params[\"b2\"]\n",
    "        N, D = X.shape # X == (N,D)\n",
    "\n",
    "        # 여기에 p를 구하는 작업을 수행하세요.\n",
    "        \n",
    "        # Linear1 계층\n",
    "        h = np.dot(X, W1) + b1  # H == (N,H)\n",
    "        \n",
    "        # ReLU 계층\n",
    "        mask = (h <= 0)\n",
    "        a = h.copy()\n",
    "        a[mask] = 0 # a == (N,H)\n",
    "        \n",
    "        # Linear2 계층\n",
    "        h2 = np.dot(a, W2) + b2 # h2 == (N,C)\n",
    "        \n",
    "        # softmax 계층\n",
    "        '''\n",
    "        def softmax(x):\n",
    "            e_x = np.exp(x - np.max(x))\n",
    "            return e_x / e_x.sum()\n",
    "        '''\n",
    "        o = np.exp(h2 - np.max(h2))\n",
    "        p = np.exp(o)/np.sum(np.exp(o),axis=1).reshape(-1,1) # p == (N,C)\n",
    "\n",
    "        if y is None:\n",
    "            return p, a\n",
    "        \n",
    "        # 여기에 Loss를 구하는 작업을 수행하세요.(y가 있는 상태)\n",
    "        log_likelihood = 0\n",
    "        i = 0\n",
    "        for q in y:\n",
    "            log_likelihood -= np.log(p[i,q])\n",
    "            i += 1\n",
    "            \n",
    "        Loss = log_likelihood / N # N은 데이터 개수\n",
    "\n",
    "        print('loss : ',Loss)\n",
    "\n",
    "        return Loss\n",
    "\n",
    "\n",
    "\n",
    "    def backward(self, X, y, learning_rate=1e-5):\n",
    "        \"\"\"\n",
    "\n",
    "        X: input 데이터 (N, D)\n",
    "        y: 레이블 (N,)\n",
    "\n",
    "        grads에는 Loss에 대한 W1, b1, W2, b2 미분 값이 기록됩니다.\n",
    "\n",
    "        원래 backw 미분 결과를 return 하지만\n",
    "        여기서는 Gradient Descent방식으로 가중치 갱신까지 합니다.\n",
    "\n",
    "        \"\"\"\n",
    "        W1, b1 = self.params[\"W1\"], self.params[\"b1\"]\n",
    "        W2, b2 = self.params[\"W2\"], self.params[\"b2\"]\n",
    "        N = X.shape[0] # 데이터 개수\n",
    "        grads = {}\n",
    "\n",
    "        p, a = self.forward(X)\n",
    "\n",
    "        # 여기에 파라미터에 대한 미분을 저장하세요.\n",
    "\n",
    "        # softmax backpropagation\n",
    "        # 요컨대 Softmax-with-Loss 노드의 역전파 그래디언트를 구하려면 입력값에 소프트맥스 확률값을 취한 뒤, 정답 레이블에 해당하는 요소만 1을 빼주면 된다는 얘기\n",
    "        dp = p\n",
    "        for i in range(p.shape[0]):\n",
    "            for j in range(p.shape[1]):\n",
    "                if(j==y[i]):\n",
    "                    dp[i][j]-=1 # p-y\n",
    "        \n",
    "        # linear2 backpropagation\n",
    "        grads[\"W2\"] = np.dot(a.T, dp)\n",
    "        grads[\"b2\"] = np.sum(dp,axis=0)\n",
    "        dl = np.dot(dp, W2.T)\n",
    "        \n",
    "        # ReLU backpropagation..?\n",
    "        '''\n",
    "                            0 if x1 < 0\n",
    "        heaviside(x1, x2) = x2 if x1 == 0\n",
    "                            1 if x1 > 0\n",
    "        '''\n",
    "        da = np.heaviside(a,0)\n",
    "\n",
    "        # linear1 backpropagation\n",
    "        dc = dl * da\n",
    "        grads[\"W1\"] = np.dot(X.T, dc)\n",
    "        grads[\"b1\"] = np.sum(dc, axis=0)\n",
    "\n",
    "        self.params[\"W2\"] -= learning_rate * grads[\"W2\"]\n",
    "        self.params[\"b2\"] -= learning_rate * grads[\"b2\"]\n",
    "        self.params[\"W1\"] -= learning_rate * grads[\"W1\"]\n",
    "        self.params[\"b1\"] -= learning_rate * grads[\"b1\"]\n",
    "\n",
    "    def accuracy(self, X, y):\n",
    "\n",
    "        p, _ = self.forward(X)\n",
    "        \n",
    "        \n",
    "        pre_p = np.argmax(p,axis=1)\n",
    "\n",
    "        return np.sum(pre_p==y)/pre_p.shape[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": [
    "a = np.array([[1,1,1],\n",
    "    [2,2,2]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1, 2],\n",
       "       [1, 2],\n",
       "       [1, 2]])"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "Collapsed": "false"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
